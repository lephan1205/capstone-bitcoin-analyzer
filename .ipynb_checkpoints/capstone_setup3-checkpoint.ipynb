{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import ciso8601\n",
    "import glob\n",
    "import pickle\n",
    "from fastparquet import write\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_vwap(df):\n",
    "    \"\"\"\n",
    "    Compute volume weighted average price.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "    \"\"\"\n",
    "    q = df['foreignNotional']\n",
    "    p = df['price']\n",
    "    vwap = np.sum(p * q) / np.sum(q)\n",
    "    df['vwap'] = vwap\n",
    "    return df\n",
    "\n",
    "\n",
    "def _ohlc(df):\n",
    "    \"\"\"\n",
    "    Compute OHLC\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: pd.DataFrame\n",
    "    \n",
    "    Output:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "    \"\"\"\n",
    "    df['open'] = df.price.iloc[0]\n",
    "    df['high'] = df.price.max()\n",
    "    df['low'] = df.price.min()\n",
    "    df['close'] = df.price.iloc[-1]\n",
    "    df['volume'] = df['size'].sum()   \n",
    "    df['trades'] = df['size'].count()\n",
    "    return df[-1:]\n",
    "\n",
    "\n",
    "def _get_ts_lag(time_series, lag=pd.Timedelta(hours=1)):\n",
    "    \"\"\"Compute time series at given lag\"\"\"\n",
    "    # find time_series[t-1] integer indices given lag\n",
    "    df0 = time_series.index.searchsorted(time_series.index - lag)\n",
    "    df0 = df0[df0 > 0]  \n",
    "    \n",
    "    # align time_series[t-1] timestamps to time_series[t] timestamps \n",
    "    df0 = pd.Series(time_series.index[df0 - 1],\n",
    "                   index=time_series.index[time_series.shape[0] - df0.shape[0] : ])\n",
    "    \n",
    "    df0 = pd.Series(time_series[df0.values].values, index=df0.index)\n",
    "    \n",
    "    return df0\n",
    "\n",
    "\n",
    "def _get_direction(prices):\n",
    "    d1 = prices.diff(1)\n",
    "    d2 = prices.diff(2)\n",
    "    td = np.where(np.isnan(d1), np.nan,\n",
    "             np.where(d1 > 0, \"PlusTick\",\n",
    "                      np.where(d1 < 0, \"MinusTick\",\n",
    "                               np.where(np.isnan(d2), np.nan,\n",
    "                                        np.where(d2 > 0, \"zeroPlusTick\", \"zeroMinusTick\")))))\n",
    "    return td\n",
    "\n",
    "def dollar_sampling(df, dollars_per_bar = 2e6):\n",
    "    \"\"\"\n",
    "    Sampling observations based on a pre-defined exchanged market value\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame \n",
    "    dollars_per_bar: pre-defined dollar amount to sample\n",
    "    \n",
    "    Output:\n",
    "    df: pd.DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # add cumulative dollar column\n",
    "    data_cm_dollar = df.assign(cmDollar=df['foreignNotional'].cumsum())\n",
    "    \n",
    "    # compute total_dollars\n",
    "    total_dollars = data_cm_dollar.cmDollar.values[-1]\n",
    "    \n",
    "    # group trade by cmDollar//dollars_per_bar as groupId\n",
    "    data_dollar_grp = data_cm_dollar.assign(grpId=lambda row: row.cmDollar // dollars_per_bar)\n",
    "    \n",
    "    # for each groupId, compute vwap, OHLC, volume, and number of trades\n",
    "    data_dollar_ohlc =  data_dollar_grp.groupby('grpId').apply(lambda x: _ohlc(_compute_vwap(x)))\n",
    "    \n",
    "    # drop index level\n",
    "    data_dollar_ohlc.index = data_dollar_ohlc.index.droplevel()\n",
    "    \n",
    "    # drop rows with duplicated index but keep the first occurence\n",
    "    data_dollar_ohlc = data_dollar_ohlc[~data_dollar_ohlc.index.duplicated(keep='first')]\n",
    "    \n",
    "    # keep columns\n",
    "    mask = ['vwap', 'open', 'high', 'low', 'close', 'volume', 'trades']\n",
    "    data_dollar_ohlc = data_dollar_ohlc[mask]\n",
    "    \n",
    "    return data_dollar_ohlc\n",
    "\n",
    "\n",
    "def _get_volatility(prices, span=100, delta=pd.Timedelta(hours=1)):\n",
    "    \"\"\"\n",
    "    Compute price return of the form p[t]/p[t-1] - 1\n",
    "    \n",
    "    Input: prices :: pd series of prices\n",
    "           span0  :: the width or lag of the ewm() filter\n",
    "           delta  :: time interval of volatility to be computed\n",
    "    Output: pd series of volatility for each given time interval\n",
    "    \"\"\"\n",
    "    \n",
    "    # find p[t-1] indices given delta\n",
    "    df0 = prices.index.searchsorted(prices.index - delta)\n",
    "    df0 = df0[df0 > 0]  \n",
    "    \n",
    "    # align p[t-1] timestamps to p[t] timestamps \n",
    "    df0 = pd.Series(prices.index[df0 - 1],\n",
    "                   index=prices.index[prices.shape[0] - df0.shape[0] : ])\n",
    "    \n",
    "    # get values for each timestamps then compute returns\n",
    "    df0 = prices.loc[df0.index] / prices.loc[df0.values].values - 1\n",
    "    \n",
    "    # estimate rolling standard deviation\n",
    "    df0 = df0.ewm(span=span).std()\n",
    "    df0 = df0[df0 != 0]\n",
    "    \n",
    "    return df0\n",
    "\n",
    "\n",
    "##==== Functions implementing Triple-Barrier Method ====\n",
    "    \n",
    "def _get_verticals(prices, delta=pd.Timedelta(hours=1)):\n",
    "    \"\"\"\n",
    "    Returns the timestamps for vertical barriers given\n",
    "    a strategy's holding period.\n",
    "    \n",
    "    Input:  prices :: pd series of prices\n",
    "            delta  :: strategy's holding period\n",
    "    Output: pd Series of timestamps for vertical barriers\n",
    "    \n",
    "    Implement code snippet 3.4 in \"Advances in Financial Machines Learning\"\n",
    "    by Marcos Lopez De Padro.    \n",
    "    \"\"\"\n",
    "    \n",
    "    # find the vertical barrier index for each timestamp\n",
    "    t1 = prices.index.searchsorted(prices.index + delta)\n",
    "    t1 = t1[t1 < prices.shape[0]] \n",
    "    \n",
    "    # retrieve the vertical barrier's timestamp\n",
    "    t1 = prices.index[t1]\n",
    "    \n",
    "    # as a series\n",
    "    t1 = pd.Series(t1, index=prices.index[:t1.shape[0]])\n",
    "    return t1\n",
    "\n",
    "def _get_horizontals(data, factors=[2, 2]):\n",
    "    \n",
    "    # events are [t1, threshold, side]\n",
    "    events = data[['t1', 'threshold']]\n",
    "    events = events.assign(side=pd.Series(1., events.index)) # long only\n",
    "    \n",
    "    out = events[['t1']].copy(deep=True)\n",
    "    \n",
    "    # set upper threshold\n",
    "    if factors[0] > 0:\n",
    "        thresh_upper = factors[0] * events['threshold']\n",
    "    else: \n",
    "        thresh_upper = pd.Series(index=events.index)         # NaN; no upper threshold\n",
    "        \n",
    "    # set lower threshold\n",
    "    if factors[1] > 0:\n",
    "        thresh_lower = -factors[1] * events['threshold']\n",
    "    else:\n",
    "        thresh_lower = pd.Series(index=events.index)         # NaN; no lower threshold\n",
    "    \n",
    "    # return the timestamp of earliest stop-loss or profit taking\n",
    "    for loc, t1 in events['t1'].iteritems():  \n",
    "        dfhi = data['high'][loc:t1]                                  # path of high prices\n",
    "        dflo = data['low'][loc:t1]                                   # path of low prices\n",
    "        dfhi = (dfhi / data['close'][loc] - 1) * events.side[loc]    # path of high returns\n",
    "        dflo = (dflo / data['close'][loc] - 1) * events.side[loc]    # path of low returns\n",
    "        out.loc[loc, 'stop_loss'] = dflo[dflo < thresh_lower[loc]].index.min()   # earliest stop loss\n",
    "        out.loc[loc, 'take_profit'] = dfhi[dfhi > thresh_upper[loc]].index.min() # earliest profit taking\n",
    "    return out\n",
    "\n",
    "def _get_labels(touches):\n",
    "    \"\"\"\n",
    "    Assigns a label in {-1, 0, 1} depending on which of the \n",
    "    three barriers is hit first. \n",
    "    \n",
    "    Input: touches:: get_horizontals(data_ohlc.close, events, [1,1]),\n",
    "                     a dataframe with three columns:\n",
    "                     t1            :: the timestamp of vertical barrier\n",
    "                     stop_loss     :: the timestamp for the lower barrier\n",
    "                     profit_taking :: the time stamp for the upper barrier\n",
    "    The result of get_horizon() will then be used to assign labels based on\n",
    "    which barrier is hit first.\n",
    "    \n",
    "    Based on Maks Ivanov's implementation of MLDP's triple-barrier labeling method.\n",
    "    \"\"\"\n",
    "    \n",
    "    out = touches.copy(deep=True)\n",
    "    # pandas df.min() ignores NaN values\n",
    "    first_touch = touches[['stop_loss', 'take_profit']].min(axis=1)\n",
    "    for loc, t in first_touch.items():\n",
    "        if pd.isnull(t):\n",
    "            out.loc[loc, 'label'] = 0\n",
    "        elif t == touches.loc[loc, 'stop_loss']:\n",
    "            out.loc[loc, 'label'] = -1\n",
    "        else:\n",
    "            out.loc[loc, 'label'] = 1\n",
    "    return out\n",
    "\n",
    "\n",
    "def assign_labels(data, holding_period=pd.Timedelta(hours=1),\\\n",
    "                  volatility_window=pd.Timedelta(hours=1), factors = [2, 2]):\n",
    "    \"\"\"\n",
    "    Implement Triple-Barrier labeling method in AFML by Marcos Lopez De Padro \n",
    "    but with modified method to compute upper and lower horizontal bounds.\n",
    "    \"\"\"\n",
    "\n",
    "    data = data.assign(tickDirection=_get_direction(data.close),\n",
    "                                   closeLag1Hr=_get_ts_lag(data.close, lag=pd.Timedelta(hours=1)))\n",
    "    data = data.assign(return1Hr=data.closeLag1Hr/data.close - 1).dropna()\n",
    "\n",
    "    # add thresholds and vertical barrier (t1) columns\n",
    "    data = data.assign(threshold=_get_volatility(data.close, delta=volatility_window), \n",
    "                             t1=_get_verticals(data, delta=holding_period)).dropna()\n",
    "\n",
    "    # events are [t1, threshold, side]\n",
    "    events = data[['t1', 'threshold']]\n",
    "    events = events.assign(side=pd.Series(1., events.index)) # long only\n",
    "\n",
    "    # get the timestamps for [t1, stop_loss, take_profit]\n",
    "    touches = _get_horizontals(data, factors)\n",
    "    # assign labels based on which barrier is hit first\n",
    "    touches = _get_labels(touches)\n",
    "\n",
    "    # add touches timestamps and label\n",
    "    data = pd.concat( [data.loc[:, 'vwap':'threshold'], \n",
    "                        touches.loc[:, 't1':'label']], axis=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sampling and Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.68080163002014\n"
     ]
    }
   ],
   "source": [
    "# Read csv files and reformat \n",
    "start = time.time()\n",
    "data = pd.concat([pd.read_csv(f) for f in glob.glob('data/*.csv')], ignore_index = True)\n",
    "data = data[data.symbol == 'XBTUSD']\n",
    "data['timestamp'] = data.timestamp.map(lambda t: ciso8601.parse_datetime(t.replace('D', ' '))) \n",
    "data.set_index('timestamp', inplace=True)\n",
    "data.sort_index(inplace=True)\n",
    "\n",
    "# sample and label data\n",
    "start = time.time()\n",
    "sampled_df = dollar_sampling(data)\n",
    "labeled_df = assign_labels(sampled_df)\n",
    "\n",
    "# save to file\n",
    "write('labeled_data.pq', labeled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150.06938004493713\n",
      "best random forrest from grid search: 0.754\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Train the classifier with the following features:\n",
    "# Numeric Features:\n",
    "# - vwap: float\n",
    "# - open: float\n",
    "# - high: float\n",
    "# - low: float\n",
    "# - close: float\n",
    "# - volume: int\n",
    "# - trades: int\n",
    "# - return1Hr: float\n",
    "# Categorical Feature:\n",
    "# - tickDirection\n",
    "\n",
    "\n",
    "# First, create the processing pipelines for both numeric and categorical data\n",
    "numeric_features = ['vwap', 'open', 'high', 'low', 'volume', 'trades', 'return1Hr']\n",
    "categorical_features = ['tickDirection']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Append classifier to preprocessing pipeline\n",
    "# Now we have a full prediction pipeline\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=140, \n",
    "                                          max_depth=11, \n",
    "                                          class_weight='balanced',\n",
    "                                          random_state=42))])\n",
    "\n",
    "\n",
    "# Select columns to build model\n",
    "dropcols = ['closeLag1Hr', 'threshold', 't1', 'stop_loss', 'take_profit']\n",
    "X = labeled_df.drop(dropcols, axis=1)\n",
    "y = labeled_df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Grid search cross validation\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': range(80, 160, 20),\n",
    "    'classifier__max_depth': range(5, 12, 2)\n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=10, iid=False)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(time.time() - start)\n",
    "\n",
    "print((\"best random forrest from grid search: %.3f\"\n",
    "       % grid_search.score(X_test, y_test)))\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(grid_search, open(filename, 'wb'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
